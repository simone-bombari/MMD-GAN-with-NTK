ucx/1.7.0 on Debian 10(buster) does not support Cuda, yet 
ucx/1.7.0 on Debian 10(buster) does not support Cuda, yet 
ucx/1.7.0 on Debian 10(buster) does not support Cuda, yet 

The following have been reloaded with a version change:
  1) python/3.7.6 => python/3.8.5

Sat Apr  3 00:42:43 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  On   | 00000000:02:00.0 Off |                  N/A |
| 21%   32C    P8     9W / 250W |      1MiB / 11178MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  On   | 00000000:03:00.0 Off |                  N/A |
| 21%   31C    P8     9W / 250W |      1MiB / 11178MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 108...  On   | 00000000:83:00.0 Off |                  N/A |
| 21%   28C    P8     9W / 250W |      1MiB / 11178MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 108...  On   | 00000000:84:00.0 Off |                  N/A |
| 21%   29C    P8     9W / 250W |      1MiB / 11178MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
epoch 0
cuda:0
cuda:0
cuda:0 

minibatch 0
loss =  0.044493794441223145
two parameters  -0.0932348221540451 -0.07557204365730286
minibatch 1
loss =  0.04319429397583008
two parameters  -0.09323254227638245 -0.07557015866041183
minibatch 2
loss =  0.04233217239379883
two parameters  -0.0932302474975586 -0.07556827366352081
minibatch 3
loss =  0.04040634632110596
two parameters  -0.09322794526815414 -0.07556638866662979
minibatch 4
loss =  0.04012727737426758
two parameters  -0.0932256430387497 -0.07556450366973877
minibatch 5
loss =  0.039238929748535156
two parameters  -0.09322334080934525 -0.07556261867284775
minibatch 6
loss =  0.03944563865661621
two parameters  -0.09322104603052139 -0.07556073367595673
minibatch 7
loss =  0.03921782970428467
two parameters  -0.09321875125169754 -0.0755588486790657
minibatch 8
loss =  0.038408756256103516
two parameters  -0.09321645647287369 -0.07555696368217468
minibatch 9
loss =  0.0383296012878418
two parameters  -0.09321416914463043 -0.07555507868528366
minibatch 10
loss =  0.037838101387023926
two parameters  -0.09321188181638718 -0.07555319368839264
minibatch 11
loss =  0.03824448585510254
two parameters  -0.09320959448814392 -0.07555130869150162
minibatch 12
loss =  0.03717482089996338
two parameters  -0.09320730715990067 -0.0755494236946106
minibatch 13
loss =  0.03822040557861328
two parameters  -0.09320501983165741 -0.07554753869771957
minibatch 14
loss =  0.037590742111206055
two parameters  -0.09320273995399475 -0.07554565370082855
minibatch 15
loss =  0.037401795387268066
two parameters  -0.09320046007633209 -0.07554376870393753
minibatch 16
loss =  0.03685891628265381
two parameters  -0.09319818764925003 -0.07554188370704651
minibatch 17
loss =  0.03642463684082031
two parameters  -0.09319590777158737 -0.07553999871015549
minibatch 18
loss =  0.03577101230621338
two parameters  -0.09319363534450531 -0.07553811371326447
minibatch 19
loss =  0.0343400239944458
two parameters  -0.09319136291742325 -0.07553622871637344
minibatch 20
loss =  0.03409254550933838
two parameters  -0.09318909794092178 -0.07553434371948242
minibatch 21
loss =  0.03373408317565918
two parameters  -0.09318796545267105 -0.07553339749574661
minibatch 22
loss =  0.0330195426940918
two parameters  -0.09318683296442032 -0.0755324512720108
minibatch 23
loss =  0.032715559005737305
two parameters  -0.09318570047616959 -0.075531505048275
minibatch 24
loss =  0.03238832950592041
two parameters  -0.09318456798791885 -0.07553055882453918
minibatch 25
loss =  0.03178298473358154
two parameters  -0.09318343549966812 -0.07552961260080338
minibatch 26
loss =  0.03186905384063721
two parameters  -0.09318230301141739 -0.07552866637706757
minibatch 27
loss =  0.03180968761444092
two parameters  -0.09318117052316666 -0.07552772015333176
minibatch 28
loss =  0.03137040138244629
two parameters  -0.09318003803491592 -0.07552677392959595
minibatch 29
loss =  0.031077980995178223
two parameters  -0.09317891299724579 -0.07552582770586014
minibatch 30
loss =  0.031275033950805664
two parameters  -0.09317778795957565 -0.07552488148212433
minibatch 31
loss =  0.030012011528015137
two parameters  -0.09317666292190552 -0.07552393525838852
minibatch 32
loss =  0.028974533081054688
two parameters  -0.09317553788423538 -0.07552298903465271
minibatch 33
loss =  0.027685999870300293
two parameters  -0.09317441284656525 -0.0755220428109169
minibatch 34
loss =  0.02680027484893799
two parameters  -0.09317328780889511 -0.07552109658718109
minibatch 35
loss =  0.025511741638183594
two parameters  -0.09317216277122498 -0.07552015036344528
minibatch 36
loss =  0.024298548698425293
two parameters  -0.09317103773355484 -0.07551920413970947
minibatch 37
loss =  0.0235520601272583
two parameters  -0.0931699126958847 -0.07551825791597366
minibatch 38
loss =  0.021983742713928223
two parameters  -0.09316878765821457 -0.07551731169223785
minibatch 39
loss =  0.021199822425842285
two parameters  -0.09316766262054443 -0.07551636546850204
minibatch 40
loss =  0.02079153060913086
two parameters  -0.0931665375828743 -0.07551541924476624
minibatch 41
loss =  0.019599080085754395
two parameters  -0.09316597878932953 -0.07551494985818863
minibatch 42
loss =  0.019634008407592773
two parameters  -0.09316541999578476 -0.07551448047161102
minibatch 43
loss =  0.019730210304260254
two parameters  -0.09316486120223999 -0.07551401108503342
minibatch 44
loss =  0.019043922424316406
two parameters  -0.09316430240869522 -0.07551354169845581
minibatch 45
loss =  0.01896190643310547
two parameters  -0.09316374361515045 -0.0755130723118782
minibatch 46
loss =  0.01790142059326172
two parameters  -0.09316318482160568 -0.0755126029253006
minibatch 47
loss =  0.018401503562927246
two parameters  -0.09316262602806091 -0.07551213353872299
minibatch 48
loss =  0.01774895191192627
two parameters  -0.09316206723451614 -0.07551166415214539
minibatch 49
loss =  0.017853498458862305
two parameters  -0.09316150844097137 -0.07551119476556778
minibatch 50
loss =  0.01790308952331543
two parameters  -0.0931609496474266 -0.07551072537899017
minibatch 51
loss =  0.017156124114990234
two parameters  -0.09316038340330124 -0.07551025599241257
minibatch 52
loss =  0.01645958423614502
two parameters  -0.09315982460975647 -0.07550978660583496
minibatch 53
loss =  0.016559720039367676
two parameters  -0.0931592658162117 -0.07550931721925735
minibatch 54
loss =  0.015295743942260742
two parameters  -0.09315869957208633 -0.07550884783267975
minibatch 55
loss =  0.015834450721740723
two parameters  -0.09315814077854156 -0.07550837844610214
minibatch 56
loss =  0.015470385551452637
two parameters  -0.0931575819849968 -0.07550790905952454
minibatch 57
loss =  0.015290021896362305
two parameters  -0.09315702319145203 -0.07550743967294693
minibatch 58
loss =  0.015066385269165039
two parameters  -0.09315646439790726 -0.07550697028636932
minibatch 59
loss =  0.01459360122680664
two parameters  -0.09315590560436249 -0.07550650089979172
minibatch 60
loss =  0.014909744262695312
two parameters  -0.09315533936023712 -0.07550603151321411
minibatch 61
loss =  0.014413595199584961
two parameters  -0.09315505623817444 -0.07550579309463501
minibatch 62
loss =  0.014419317245483398
two parameters  -0.09315477311611176 -0.07550555467605591
minibatch 63
loss =  0.014865636825561523
two parameters  -0.09315448999404907 -0.0755053162574768
minibatch 64
loss =  0.014935731887817383
two parameters  -0.09315420687198639 -0.0755050778388977
minibatch 65
loss =  0.014349102973937988
two parameters  -0.0931539237499237 -0.0755048394203186
minibatch 66
loss =  0.014753341674804688
two parameters  -0.09315364062786102 -0.0755046010017395
minibatch 67
loss =  0.014011502265930176
two parameters  -0.09315335750579834 -0.0755043625831604
minibatch 68
loss =  0.014227509498596191
two parameters  -0.09315307438373566 -0.0755041241645813
minibatch 69
loss =  0.014042019844055176
two parameters  -0.09315279126167297 -0.0755038857460022
minibatch 70
loss =  0.01403343677520752
two parameters  -0.09315250813961029 -0.0755036473274231
minibatch 71
loss =  0.013829827308654785
two parameters  -0.09315222501754761 -0.075503408908844
minibatch 72
loss =  0.013801336288452148
two parameters  -0.09315194189548492 -0.07550317049026489
minibatch 73
loss =  0.01357269287109375
two parameters  -0.09315165877342224 -0.07550293207168579
minibatch 74
loss =  0.013261198997497559
two parameters  -0.09315137565135956 -0.07550269365310669
minibatch 75
loss =  0.013887166976928711
two parameters  -0.09315109252929688 -0.07550245523452759
minibatch 76
loss =  0.013250350952148438
two parameters  -0.09315080940723419 -0.07550221681594849
minibatch 77
loss =  0.0139390230178833
two parameters  -0.09315052628517151 -0.07550197839736938
minibatch 78
loss =  0.01313626766204834
two parameters  -0.09315024316310883 -0.07550173997879028
minibatch 79
loss =  0.012941479682922363
two parameters  -0.09314996004104614 -0.07550150156021118
minibatch 80
loss =  0.012936949729919434
two parameters  -0.09314967691898346 -0.07550126314163208
minibatch 81
loss =  0.013215899467468262
two parameters  -0.09314953535795212 -0.07550114393234253
minibatch 82
loss =  0.013174891471862793
two parameters  -0.09314939379692078 -0.07550102472305298
minibatch 83
loss =  0.013087987899780273
two parameters  -0.09314925223588943 -0.07550090551376343
minibatch 84
loss =  0.012801051139831543
two parameters  -0.0931491106748581 -0.07550078630447388
minibatch 85
loss =  0.012534499168395996
two parameters  -0.09314896911382675 -0.07550066709518433
minibatch 86
loss =  0.013405323028564453
two parameters  -0.09314882755279541 -0.07550054788589478
minibatch 87
loss =  0.012994647026062012
two parameters  -0.09314868599176407 -0.07550042867660522
minibatch 88
loss =  0.012610554695129395
two parameters  -0.09314854443073273 -0.07550030946731567
minibatch 89
loss =  0.012511491775512695
two parameters  -0.09314840286970139 -0.07550019025802612
minibatch 90
loss =  0.012617707252502441
two parameters  -0.09314826130867004 -0.07550007104873657
minibatch 91
loss =  0.013031005859375
two parameters  -0.0931481197476387 -0.07549995183944702
minibatch 92
loss =  0.012507438659667969
two parameters  -0.09314797818660736 -0.07549983263015747
minibatch 93
loss =  0.012534856796264648
two parameters  -0.09314783662557602 -0.07549971342086792
minibatch 94
loss =  0.01262354850769043
two parameters  -0.09314769506454468 -0.07549959421157837
minibatch 95
loss =  0.012243866920471191
two parameters  -0.09314755350351334 -0.07549947500228882
minibatch 96
loss =  0.012308716773986816
two parameters  -0.093147411942482 -0.07549935579299927
minibatch 97
loss =  0.012243032455444336
two parameters  -0.09314727038145065 -0.07549923658370972
minibatch 98
loss =  0.012267470359802246
two parameters  -0.09314712882041931 -0.07549911737442017
minibatch 99
loss =  0.012404084205627441
two parameters  -0.09314698725938797 -0.07549899816513062
minibatch 100
loss =  0.012427330017089844
two parameters  -0.09314684569835663 -0.07549887895584106
minibatch 101
loss =  0.012553215026855469
two parameters  -0.09314677119255066 -0.07549881935119629
minibatch 102
loss =  0.011452078819274902
two parameters  -0.09314669668674469 -0.07549875974655151
minibatch 103
loss =  0.012409806251525879
two parameters  -0.09314662963151932 -0.07549870014190674
minibatch 104
loss =  0.012082934379577637
two parameters  -0.09314656257629395 -0.07549864053726196
minibatch 105
loss =  0.011956572532653809
two parameters  -0.09314649552106857 -0.07549858093261719
minibatch 106
loss =  0.012717008590698242
two parameters  -0.0931464284658432 -0.07549852132797241
minibatch 107
loss =  0.01236879825592041
two parameters  -0.09314635396003723 -0.07549846172332764
minibatch 108
loss =  0.012038946151733398
two parameters  -0.09314628690481186 -0.07549840211868286
minibatch 109
loss =  0.011824965476989746
two parameters  -0.09314621239900589 -0.07549834251403809
