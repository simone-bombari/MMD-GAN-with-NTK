ucx/1.7.0 on Debian 10(buster) does not support Cuda, yet 
ucx/1.7.0 on Debian 10(buster) does not support Cuda, yet 
ucx/1.7.0 on Debian 10(buster) does not support Cuda, yet 

The following have been reloaded with a version change:
  1) python/3.7.6 => python/3.8.5

Sat Apr  3 00:43:46 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 108...  On   | 00000000:02:00.0 Off |                  N/A |
| 21%   42C    P2    59W / 250W |   1903MiB / 11178MiB |     12%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 108...  On   | 00000000:03:00.0 Off |                  N/A |
| 21%   31C    P8     9W / 250W |      4MiB / 11178MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 108...  On   | 00000000:83:00.0 Off |                  N/A |
| 21%   28C    P8     9W / 250W |      1MiB / 11178MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 108...  On   | 00000000:84:00.0 Off |                  N/A |
| 21%   29C    P8     9W / 250W |      1MiB / 11178MiB |      0%   E. Process |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A     34456      C   .../python/3.8.5/bin/python3     1899MiB |
+-----------------------------------------------------------------------------+
epoch 0
cuda:0
cuda:0
cuda:0 

minibatch 0
loss =  0.03895747661590576
two parameters  0.09886851906776428 0.004747368395328522
minibatch 1
loss =  0.010365605354309082
two parameters  -0.15088576078414917 -0.24525035917758942
minibatch 2
loss =  0.011077046394348145
two parameters  -0.09020061045885086 -0.4085766673088074
minibatch 3
loss =  0.010975360870361328
two parameters  0.022736806422472 -0.5288148522377014
minibatch 4
loss =  0.011056065559387207
two parameters  0.09849400818347931 -0.6202199459075928
minibatch 5
loss =  0.011298060417175293
two parameters  0.09692728519439697 -0.6896872520446777
minibatch 6
loss =  0.010937929153442383
two parameters  0.04437120631337166 -0.7412920594215393
minibatch 7
loss =  0.010732769966125488
two parameters  -0.02322673797607422 -0.7777764797210693
minibatch 8
loss =  0.010887861251831055
two parameters  -0.07110763341188431 -0.8011658787727356
minibatch 9
loss =  0.010772347450256348
two parameters  -0.0787331834435463 -0.813064455986023
minibatch 10
loss =  0.010683894157409668
two parameters  -0.0502789132297039 -0.8148118853569031
minibatch 11
loss =  0.009861469268798828
two parameters  -0.0033375173807144165 -0.8075724840164185
minibatch 12
loss =  0.010342836380004883
two parameters  0.0403137281537056 -0.7923883199691772
minibatch 13
loss =  0.011074542999267578
two parameters  0.061894096434116364 -0.7702127695083618
minibatch 14
loss =  0.011264443397521973
two parameters  0.05481043830513954 -0.7419314384460449
minibatch 15
loss =  0.011018037796020508
two parameters  0.02567421831190586 -0.7083764672279358
minibatch 16
loss =  0.011115789413452148
two parameters  -0.01135849766433239 -0.6703352928161621
minibatch 17
loss =  0.0103834867477417
two parameters  -0.0401465930044651 -0.6285567283630371
minibatch 18
loss =  0.09257841110229492
two parameters  -0.04930862784385681 -0.5837545394897461
minibatch 19
loss =  0.01072227954864502
two parameters  -0.11250435560941696 -0.5366095900535583
minibatch 20
loss =  0.01073598861694336
two parameters  -0.12592704594135284 -0.487770676612854
minibatch 21
loss =  0.010046005249023438
two parameters  -0.10988444834947586 -0.4628126323223114
minibatch 22
loss =  0.010454177856445312
two parameters  -0.07746133208274841 -0.437467485666275
minibatch 23
loss =  0.010723710060119629
two parameters  -0.03563603013753891 -0.41187864542007446
minibatch 24
loss =  0.010175347328186035
two parameters  0.008271808736026287 -0.38618165254592896
minibatch 25
loss =  0.010466694831848145
two parameters  0.04685447737574577 -0.3605043888092041
minibatch 26
loss =  0.01068568229675293
two parameters  0.07376988977193832 -0.3349672555923462
minibatch 27
loss =  0.010692358016967773
two parameters  0.08536288887262344 -0.309683233499527
minibatch 28
loss =  0.010501623153686523
two parameters  0.08135320991277695 -0.2847578823566437
minibatch 29
loss =  0.00974881649017334
two parameters  0.06419287621974945 -0.26028937101364136
minibatch 30
loss =  0.010363340377807617
two parameters  0.03801780939102173 -0.23636843264102936
minibatch 31
loss =  0.01052546501159668
two parameters  0.007864724844694138 -0.2130783498287201
minibatch 32
loss =  0.01025092601776123
two parameters  -0.02094017155468464 -0.19049493968486786
minibatch 33
loss =  0.010437846183776855
two parameters  -0.04354042559862137 -0.16868653893470764
minibatch 34
loss =  0.01037752628326416
two parameters  -0.05646762624382973 -0.1477140188217163
minibatch 35
loss =  0.010663866996765137
two parameters  -0.058314595371484756 -0.127630814909935
minibatch 36
loss =  0.010953903198242188
two parameters  -0.04980706423521042 -0.1084829643368721
minibatch 37
loss =  0.010096907615661621
two parameters  -0.033376213163137436 -0.09030920267105103
minibatch 38
loss =  0.011085033416748047
two parameters  -0.012556850910186768 -0.07314106822013855
minibatch 39
loss =  0.010961055755615234
two parameters  0.00863257423043251 -0.057003043591976166
minibatch 40
loss =  0.010614991188049316
two parameters  0.026353657245635986 -0.04191272333264351
minibatch 41
loss =  0.009953737258911133
two parameters  0.0319976806640625 -0.03489686921238899
minibatch 42
loss =  0.010200142860412598
two parameters  0.03417542576789856 -0.028387270867824554
minibatch 43
loss =  0.009977221488952637
two parameters  0.03298158571124077 -0.02236154116690159
minibatch 44
loss =  0.010872483253479004
two parameters  0.028816090896725655 -0.01679791510105133
minibatch 45
loss =  0.010823369026184082
two parameters  0.02231970988214016 -0.011675226502120495
minibatch 46
loss =  0.010226130485534668
two parameters  0.014296840876340866 -0.006972878240048885
minibatch 47
loss =  0.010884761810302734
two parameters  0.005633057095110416 -0.002670824062079191
minibatch 48
loss =  0.010625958442687988
two parameters  -0.00278886198066175 0.001250449102371931
minibatch 49
loss =  0.010794878005981445
two parameters  -0.010170692577958107 0.004809933714568615
minibatch 50
loss =  0.010663747787475586
two parameters  -0.015871919691562653 0.008026114664971828
minibatch 51
loss =  0.010836243629455566
two parameters  -0.019465496763586998 0.01091697532683611
minibatch 52
loss =  0.01098930835723877
two parameters  -0.02076837047934532 0.013500005938112736
minibatch 53
loss =  0.01035153865814209
two parameters  -0.01984323188662529 0.015792207792401314
minibatch 54
loss =  0.010549068450927734
two parameters  -0.016973594203591347 0.017810098826885223
minibatch 55
loss =  0.010721087455749512
two parameters  -0.012618180364370346 0.01956971362233162
minibatch 56
loss =  0.009693384170532227
two parameters  -0.007351763546466827 0.02108660712838173
minibatch 57
loss =  0.01068413257598877
two parameters  -0.0017990667838603258 0.02237585559487343
minibatch 58
loss =  0.010621786117553711
two parameters  0.0034323888830840588 0.02345205843448639
minibatch 59
loss =  0.011004090309143066
two parameters  0.00781453587114811 0.02432934194803238
minibatch 60
loss =  0.010085701942443848
two parameters  0.010949906893074512 0.025021353736519814
minibatch 61
loss =  0.010698080062866211
two parameters  0.011777957901358604 0.025281310081481934
minibatch 62
loss =  0.010108709335327148
two parameters  0.011884881183505058 0.025462135672569275
minibatch 63
loss =  0.01062023639678955
two parameters  0.011326765641570091 0.02557029202580452
minibatch 64
loss =  0.010398030281066895
two parameters  0.010191474109888077 0.025611814111471176
minibatch 65
loss =  0.010222673416137695
two parameters  0.008591515943408012 0.025592338293790817
minibatch 66
loss =  0.011679649353027344
two parameters  0.006656094919890165 0.02551713027060032
minibatch 67
loss =  0.010832071304321289
two parameters  0.004522812552750111 0.02539110742509365
minibatch 68
loss =  0.010455131530761719
two parameters  0.0023294624406844378 0.02521885745227337
minibatch 69
loss =  0.010695219039916992
two parameters  0.00020633538952097297 0.025004664435982704
minibatch 70
loss =  0.009690999984741211
two parameters  -0.0017305905930697918 0.024752525612711906
minibatch 71
loss =  0.010248541831970215
two parameters  -0.003385242773219943 0.024466168135404587
minibatch 72
loss =  0.010808110237121582
two parameters  -0.004685563035309315 0.02414907142519951
minibatch 73
loss =  0.010756850242614746
two parameters  -0.005585960578173399 0.023804476484656334
minibatch 74
loss =  0.01065075397491455
two parameters  -0.006068046670407057 0.023435402661561966
minibatch 75
loss =  0.010258316993713379
two parameters  -0.006139697972685099 0.023044664412736893
minibatch 76
loss =  0.01046597957611084
two parameters  -0.005832619499415159 0.022634882479906082
minibatch 77
loss =  0.010223984718322754
two parameters  -0.005198649130761623 0.02220849320292473
minibatch 78
loss =  0.010326147079467773
two parameters  -0.004305118694901466 0.021767763420939445
minibatch 79
loss =  0.010488390922546387
two parameters  -0.0032296127174049616 0.021314799785614014
minibatch 80
loss =  0.010855555534362793
two parameters  -0.0020544789731502533 0.020851558074355125
minibatch 81
loss =  0.010433673858642578
two parameters  -0.0014579559210687876 0.020615704357624054
minibatch 82
loss =  0.010652542114257812
two parameters  -0.0008717386517673731 0.020376164466142654
minibatch 83
loss =  0.011512041091918945
two parameters  -0.00031318521359935403 0.02013343945145607
minibatch 84
loss =  0.010349869728088379
two parameters  0.00020263534679543227 0.01988799311220646
minibatch 85
loss =  0.01079249382019043
two parameters  0.0006632315926253796 0.019640253856778145
minibatch 86
loss =  0.011001229286193848
two parameters  0.0010588793084025383 0.019390618428587914
minibatch 87
loss =  0.01053774356842041
two parameters  0.0013827119255438447 0.019139450043439865
minibatch 88
loss =  0.010493278503417969
two parameters  0.0016307152109220624 0.01888708770275116
minibatch 89
loss =  0.010954022407531738
two parameters  0.0018016328103840351 0.01863384060561657
minibatch 90
loss =  0.009451031684875488
two parameters  0.0018967916257679462 0.018379997462034225
minibatch 91
loss =  0.0106353759765625
two parameters  0.0019198573427274823 0.01812582276761532
minibatch 92
loss =  0.01051187515258789
two parameters  0.0018765325658023357 0.017871560528874397
minibatch 93
loss =  0.010081052780151367
two parameters  0.0017742121126502752 0.01761743798851967
minibatch 94
loss =  0.010147333145141602
two parameters  0.001621608273126185 0.017363663762807846
minibatch 95
loss =  0.01029980182647705
two parameters  0.0014283610507845879 0.01711042784154415
minibatch 96
loss =  0.010151267051696777
two parameters  0.0012046466581523418 0.01685790903866291
minibatch 97
loss =  0.010530591011047363
two parameters  0.0009607976535335183 0.016606271266937256
minibatch 98
loss =  0.01048588752746582
two parameters  0.0007069456623867154 0.016355665400624275
minibatch 99
loss =  0.011269688606262207
two parameters  0.0004526975972112268 0.01610622927546501
minibatch 100
loss =  0.01096951961517334
two parameters  0.00020685391791630536 0.015858091413974762
minibatch 101
loss =  0.010827183723449707
two parameters  9.201424109051004e-05 0.015734730288386345
minibatch 102
loss =  0.010614395141601562
two parameters  -1.3500634850061033e-05 0.015612044371664524
minibatch 103
loss =  0.009921669960021973
two parameters  -0.00010871187987504527 0.015490003861486912
minibatch 104
loss =  0.010640859603881836
two parameters  -0.00019289959163870662 0.015368583612143993
minibatch 105
loss =  0.0110548734664917
two parameters  -0.00026559396064840257 0.015247760340571404
minibatch 106
loss =  0.010774970054626465
two parameters  -0.00032656293478794396 0.0151275135576725
minibatch 107
loss =  0.009665608406066895
two parameters  -0.0003757969243451953 0.015007825568318367
minibatch 108
loss =  0.010282278060913086
two parameters  -0.0004134909249842167 0.014888681471347809
minibatch 109
loss =  0.010396480560302734
two parameters  -0.00044002450886182487 0.014770067296922207
