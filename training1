ucx/1.7.0 on Debian 10(buster) does not support Cuda, yet 
ucx/1.7.0 on Debian 10(buster) does not support Cuda, yet 
ucx/1.7.0 on Debian 10(buster) does not support Cuda, yet 

The following have been reloaded with a version change:
  1) python/3.7.6 => python/3.8.5

Sat Apr  3 00:39:50 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 980     Off  | 00000000:02:00.0 Off |                  N/A |
| 28%   24C    P8    11W / 180W |     14MiB /  4043MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 980     Off  | 00000000:03:00.0 Off |                  N/A |
| 35%   22C    P8    11W / 180W |      2MiB /  4043MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 980     Off  | 00000000:82:00.0 Off |                  N/A |
| 27%   21C    P8    12W / 180W |      2MiB /  4043MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 980     Off  | 00000000:83:00.0 Off |                  N/A |
| 26%   21C    P8    13W / 180W |      2MiB /  4043MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1855      G   /usr/lib/xorg/Xorg                 11MiB |
+-----------------------------------------------------------------------------+
epoch 0
cuda:0
cuda:0
cuda:0 

minibatch 0
loss =  0.04393637180328369
two parameters  -0.0465567409992218 0.0992349311709404
minibatch 1
loss =  0.04269826412200928
two parameters  -0.046555496752262115 0.09914363920688629
minibatch 2
loss =  0.042137861251831055
two parameters  -0.046554259955883026 0.09908116608858109
minibatch 3
loss =  0.040860772132873535
two parameters  -0.04655302315950394 0.09903603792190552
minibatch 4
loss =  0.038852691650390625
two parameters  -0.046551793813705444 0.0990019366145134
minibatch 5
loss =  0.039104342460632324
two parameters  -0.04655057191848755 0.09897516667842865
minibatch 6
loss =  0.0388871431350708
two parameters  -0.04654935374855995 0.09895320981740952
minibatch 7
loss =  0.03812766075134277
two parameters  -0.046548135578632355 0.09893499314785004
minibatch 8
loss =  0.038550376892089844
two parameters  -0.046546921133995056 0.09891962260007858
minibatch 9
loss =  0.03859281539916992
two parameters  -0.04654570668935776 0.09890636056661606
minibatch 10
loss =  0.037865519523620605
two parameters  -0.046544499695301056 0.098894864320755
minibatch 11
loss =  0.03786945343017578
two parameters  -0.046543292701244354 0.09888472408056259
minibatch 12
loss =  0.03801870346069336
two parameters  -0.04654208943247795 0.09887570142745972
minibatch 13
loss =  0.03737759590148926
two parameters  -0.04654088616371155 0.09886758029460907
minibatch 14
loss =  0.03687942028045654
two parameters  -0.04653968662023544 0.0988602340221405
minibatch 15
loss =  0.03664898872375488
two parameters  -0.04653848707675934 0.09885354340076447
minibatch 16
loss =  0.03647267818450928
two parameters  -0.04653729125857353 0.098847396671772
minibatch 17
loss =  0.03551340103149414
two parameters  -0.046536095440387726 0.09884171187877655
minibatch 18
loss =  0.035176634788513184
two parameters  -0.04653490334749222 0.09883643686771393
minibatch 19
loss =  0.03305768966674805
two parameters  -0.04653370752930641 0.09883150458335876
minibatch 20
loss =  0.033214688301086426
two parameters  -0.0465325191617012 0.09882689267396927
minibatch 21
loss =  0.03232705593109131
two parameters  -0.04653192311525345 0.09882471710443497
minibatch 22
loss =  0.03294634819030762
two parameters  -0.04653133079409599 0.09882260859012604
minibatch 23
loss =  0.03218233585357666
two parameters  -0.04653073847293854 0.09882055222988129
minibatch 24
loss =  0.03207862377166748
two parameters  -0.046530142426490784 0.09881855547428131
minibatch 25
loss =  0.031397342681884766
two parameters  -0.04652955010533333 0.09881660342216492
minibatch 26
loss =  0.031435608863830566
two parameters  -0.04652895778417587 0.0988146960735321
minibatch 27
loss =  0.031984567642211914
two parameters  -0.04652836546301842 0.09881283342838287
minibatch 28
loss =  0.03063070774078369
two parameters  -0.04652777314186096 0.09881100803613663
minibatch 29
loss =  0.030769705772399902
two parameters  -0.046527180820703506 0.09880921989679337
minibatch 30
loss =  0.030462265014648438
two parameters  -0.04652658849954605 0.09880746901035309
minibatch 31
loss =  0.029984354972839355
two parameters  -0.046525999903678894 0.0988057479262352
minibatch 32
loss =  0.02859818935394287
two parameters  -0.04652540758252144 0.0988040491938591
minibatch 33
loss =  0.02731335163116455
two parameters  -0.04652481898665428 0.09880238026380539
minibatch 34
loss =  0.026062369346618652
two parameters  -0.046524230390787125 0.09880073368549347
minibatch 35
loss =  0.024274230003356934
two parameters  -0.04652363806962967 0.09879910945892334
minibatch 36
loss =  0.024666547775268555
two parameters  -0.04652304947376251 0.0987975001335144
minibatch 37
loss =  0.02288198471069336
two parameters  -0.046522460877895355 0.09879591315984726
minibatch 38
loss =  0.022225499153137207
two parameters  -0.0465218722820282 0.09879434108734131
minibatch 39
loss =  0.021274209022521973
two parameters  -0.04652127996087074 0.09879278391599655
minibatch 40
loss =  0.020635008811950684
two parameters  -0.046520691365003586 0.09879124164581299
minibatch 41
loss =  0.020497798919677734
two parameters  -0.04652039706707001 0.0987904742360115
minibatch 42
loss =  0.020743131637573242
two parameters  -0.04652010276913643 0.09878971427679062
minibatch 43
loss =  0.02028524875640869
two parameters  -0.04651980847120285 0.09878895431756973
minibatch 44
loss =  0.019383549690246582
two parameters  -0.04651951417326927 0.09878820180892944
minibatch 45
loss =  0.019019126892089844
two parameters  -0.04651921987533569 0.09878744930028915
minibatch 46
loss =  0.018471121788024902
two parameters  -0.046518925577402115 0.09878669679164886
minibatch 47
loss =  0.018516063690185547
two parameters  -0.046518631279468536 0.09878595173358917
minibatch 48
loss =  0.018003225326538086
two parameters  -0.04651833698153496 0.09878520667552948
minibatch 49
loss =  0.018274664878845215
two parameters  -0.04651804268360138 0.09878446906805038
minibatch 50
loss =  0.017532825469970703
two parameters  -0.0465177483856678 0.09878373146057129
minibatch 51
loss =  0.017237544059753418
two parameters  -0.04651745408773422 0.0987829938530922
minibatch 52
loss =  0.01678299903869629
two parameters  -0.046517159789800644 0.0987822636961937
minibatch 53
loss =  0.016678333282470703
two parameters  -0.046516865491867065 0.0987815335392952
minibatch 54
loss =  0.01602315902709961
two parameters  -0.04651657119393349 0.0987808033823967
minibatch 55
loss =  0.01634502410888672
two parameters  -0.04651627689599991 0.0987800806760788
minibatch 56
loss =  0.01679098606109619
two parameters  -0.04651598259806633 0.0987793579697609
minibatch 57
loss =  0.01629960536956787
two parameters  -0.04651568830013275 0.098778635263443
minibatch 58
loss =  0.015942811965942383
two parameters  -0.04651539400219917 0.09877791255712509
minibatch 59
loss =  0.01584184169769287
two parameters  -0.046515099704265594 0.09877719730138779
minibatch 60
loss =  0.01566910743713379
two parameters  -0.046514805406332016 0.09877648204565048
minibatch 61
loss =  0.015617847442626953
two parameters  -0.04651465639472008 0.09877612441778183
minibatch 62
loss =  0.015236973762512207
two parameters  -0.04651450738310814 0.09877576678991318
minibatch 63
loss =  0.015490174293518066
two parameters  -0.0465143583714962 0.09877540916204453
minibatch 64
loss =  0.015102505683898926
two parameters  -0.04651420935988426 0.09877505153417587
minibatch 65
loss =  0.015413880348205566
two parameters  -0.046514060348272324 0.09877469390630722
minibatch 66
loss =  0.014994621276855469
two parameters  -0.046513911336660385 0.09877433627843857
minibatch 67
loss =  0.01557624340057373
two parameters  -0.04651376232504845 0.09877397865056992
minibatch 68
loss =  0.014881730079650879
two parameters  -0.04651361331343651 0.09877362847328186
minibatch 69
loss =  0.014657855033874512
two parameters  -0.04651346430182457 0.0987732782959938
minibatch 70
loss =  0.014248967170715332
two parameters  -0.04651331529021263 0.09877292811870575
minibatch 71
loss =  0.014711141586303711
two parameters  -0.04651316627860069 0.0987725779414177
minibatch 72
loss =  0.01447761058807373
two parameters  -0.046513017266988754 0.09877222776412964
minibatch 73
loss =  0.01430368423461914
two parameters  -0.046512868255376816 0.09877187758684158
minibatch 74
loss =  0.014139413833618164
two parameters  -0.04651271924376488 0.09877152740955353
minibatch 75
loss =  0.014520406723022461
two parameters  -0.04651257023215294 0.09877117723226547
minibatch 76
loss =  0.013711094856262207
two parameters  -0.046512421220541 0.09877082705497742
minibatch 77
loss =  0.014320135116577148
two parameters  -0.04651227220892906 0.09877047687768936
minibatch 78
loss =  0.013820886611938477
two parameters  -0.04651212319731712 0.0987701267004013
minibatch 79
loss =  0.013767242431640625
two parameters  -0.046511974185705185 0.09876977652311325
minibatch 80
loss =  0.013773441314697266
two parameters  -0.046511828899383545 0.0987694263458252
minibatch 81
loss =  0.013672709465026855
two parameters  -0.046511754393577576 0.09876925498247147
minibatch 82
loss =  0.013577938079833984
two parameters  -0.046511679887771606 0.09876908361911774
minibatch 83
loss =  0.013224959373474121
two parameters  -0.04651160538196564 0.09876891225576401
minibatch 84
loss =  0.013808965682983398
two parameters  -0.04651153087615967 0.09876874089241028
minibatch 85
loss =  0.013883113861083984
two parameters  -0.0465114563703537 0.09876856952905655
minibatch 86
loss =  0.013750553131103516
two parameters  -0.04651138186454773 0.09876839816570282
minibatch 87
loss =  0.013537764549255371
two parameters  -0.04651130735874176 0.09876822680234909
minibatch 88
loss =  0.013542413711547852
two parameters  -0.04651123285293579 0.09876805543899536
minibatch 89
loss =  0.013379812240600586
two parameters  -0.04651115834712982 0.09876788407564163
minibatch 90
loss =  0.013902068138122559
two parameters  -0.04651108384132385 0.0987677127122879
minibatch 91
loss =  0.012907862663269043
two parameters  -0.04651100933551788 0.09876754134893417
minibatch 92
loss =  0.01334691047668457
two parameters  -0.046510934829711914 0.09876736998558044
minibatch 93
loss =  0.013128995895385742
two parameters  -0.046510860323905945 0.09876719862222672
minibatch 94
loss =  0.013145685195922852
two parameters  -0.046510785818099976 0.09876702725887299
minibatch 95
loss =  0.013600587844848633
two parameters  -0.046510711312294006 0.09876685589551926
minibatch 96
loss =  0.013346314430236816
two parameters  -0.04651063680648804 0.09876668453216553
minibatch 97
loss =  0.013501405715942383
two parameters  -0.04651056230068207 0.0987665131688118
minibatch 98
loss =  0.013283371925354004
two parameters  -0.0465104877948761 0.09876634180545807
minibatch 99
loss =  0.013062596321105957
two parameters  -0.04651041328907013 0.09876617044210434
minibatch 100
loss =  0.01338207721710205
two parameters  -0.04651033878326416 0.09876599907875061
minibatch 101
loss =  0.013607978820800781
two parameters  -0.046510301530361176 0.09876590967178345
minibatch 102
loss =  0.012706875801086426
two parameters  -0.04651026427745819 0.09876582026481628
minibatch 103
loss =  0.012900352478027344
two parameters  -0.046510227024555206 0.09876573085784912
minibatch 104
loss =  0.012605071067810059
two parameters  -0.04651018977165222 0.09876564145088196
minibatch 105
loss =  0.01316225528717041
two parameters  -0.04651015251874924 0.0987655520439148
minibatch 106
loss =  0.012700796127319336
two parameters  -0.04651011526584625 0.09876546263694763
minibatch 107
loss =  0.01299440860748291
two parameters  -0.04651007801294327 0.09876537322998047
minibatch 108
loss =  0.013128995895385742
two parameters  -0.04651004076004028 0.0987652838230133
minibatch 109
loss =  0.013335108757019043
two parameters  -0.0465100035071373 0.09876519441604614
