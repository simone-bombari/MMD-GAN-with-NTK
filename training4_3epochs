ucx/1.7.0 on Debian 10(buster) does not support Cuda, yet 
ucx/1.7.0 on Debian 10(buster) does not support Cuda, yet 
ucx/1.7.0 on Debian 10(buster) does not support Cuda, yet 

The following have been reloaded with a version change:
  1) python/3.7.6 => python/3.8.5

Wed Apr  7 23:36:08 2021       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 460.39       Driver Version: 460.39       CUDA Version: 11.2     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  GeForce GTX 980     Off  | 00000000:02:00.0 Off |                  N/A |
| 30%   23C    P8    11W / 180W |     14MiB /  4043MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   1  GeForce GTX 980     Off  | 00000000:03:00.0 Off |                  N/A |
| 34%   22C    P8    11W / 180W |      2MiB /  4043MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   2  GeForce GTX 980     Off  | 00000000:82:00.0 Off |                  N/A |
| 26%   21C    P8    12W / 180W |      2MiB /  4043MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
|   3  GeForce GTX 980     Off  | 00000000:83:00.0 Off |                  N/A |
| 26%   21C    P8    13W / 180W |      2MiB /  4043MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|    0   N/A  N/A      1855      G   /usr/lib/xorg/Xorg                 11MiB |
+-----------------------------------------------------------------------------+
epoch 0
torch.Size([512, 50])
cpu
cpu
cpu 

minibatch 0
loss =  1.0963184833526611
two parameters  -0.04645740985870361 -0.09121783822774887
minibatch 1
loss =  0.7067909836769104
two parameters  0.0530434176325798 -0.19121775031089783
minibatch 2
loss =  0.5581917762756348
two parameters  0.1276630163192749 -0.27431824803352356
minibatch 3
loss =  0.5548241138458252
two parameters  0.18521350622177124 -0.33853471279144287
minibatch 4
loss =  0.5381410121917725
two parameters  0.2321871519088745 -0.39111995697021484
minibatch 5
loss =  0.5402604341506958
two parameters  0.2716926336288452 -0.4355480372905731
minibatch 6
loss =  0.5619626641273499
two parameters  0.3055194616317749 -0.47381967306137085
minibatch 7
loss =  0.554603099822998
two parameters  0.334818571805954 -0.5072216987609863
minibatch 8
loss =  0.5501092672348022
two parameters  0.36038777232170105 -0.5366458892822266
minibatch 9
loss =  0.5447083115577698
two parameters  0.3828107714653015 -0.5627440810203552
minibatch 10
loss =  0.5540021061897278
two parameters  0.4025324285030365 -0.5860125422477722
minibatch 11
loss =  0.5346505045890808
two parameters  0.4199027419090271 -0.606840968132019
minibatch 12
loss =  0.534540593624115
two parameters  0.43520423769950867 -0.6255432963371277
minibatch 13
loss =  0.5434986352920532
two parameters  0.44866976141929626 -0.6423773169517517
minibatch 14
loss =  0.5333637595176697
two parameters  0.4604945182800293 -0.6575585603713989
minibatch 15
loss =  0.5546292662620544
two parameters  0.4708445966243744 -0.6712695360183716
minibatch 16
loss =  0.5292912721633911
two parameters  0.4798629581928253 -0.6836666464805603
minibatch 17
loss =  0.5431445837020874
two parameters  0.4876739978790283 -0.69488525390625
minibatch 18
loss =  0.5437132120132446
two parameters  0.4943868815898895 -0.705043375492096
minibatch 19
loss =  0.5526726841926575
two parameters  0.5000981688499451 -0.7142447829246521
minibatch 20
loss =  0.5588613748550415
two parameters  0.5048937797546387 -0.7225812077522278
minibatch 21
loss =  0.5408745408058167
two parameters  0.5088506937026978 -0.7301340103149414
minibatch 22
loss =  0.5607355833053589
two parameters  0.5120381116867065 -0.736975908279419
minibatch 23
loss =  0.5648558735847473
two parameters  0.5145186185836792 -0.7431719303131104
minibatch 24
loss =  0.5509350299835205
two parameters  0.5163489580154419 -0.7487806081771851
minibatch 25
loss =  0.5500005483627319
two parameters  0.5175808072090149 -0.7538545727729797
minibatch 26
loss =  0.5512600541114807
two parameters  0.5182612538337708 -0.7584413886070251
minibatch 27
loss =  0.5460830926895142
two parameters  0.5184334516525269 -0.7625840902328491
minibatch 28
loss =  0.5382874011993408
two parameters  0.5181369185447693 -0.7663216590881348
minibatch 29
loss =  0.5521396398544312
two parameters  0.517408013343811 -0.7696895003318787
minibatch 30
loss =  0.5552588701248169
two parameters  0.5162801742553711 -0.7727198004722595
minibatch 31
loss =  0.5516818165779114
two parameters  0.5147842764854431 -0.7754418253898621
minibatch 32
loss =  0.5557237863540649
two parameters  0.5129488110542297 -0.7778822183609009
minibatch 33
loss =  0.5350354313850403
two parameters  0.5108000636100769 -0.7800652384757996
minibatch 34
loss =  0.5521453619003296
two parameters  0.5083624720573425 -0.7820130586624146
minibatch 35
loss =  0.5413806438446045
two parameters  0.5056585669517517 -0.7837458252906799
minibatch 36
loss =  0.5348442792892456
two parameters  0.5027093291282654 -0.7852820158004761
minibatch 37
loss =  0.5508699417114258
two parameters  0.49953415989875793 -0.7866384387016296
minibatch 38
loss =  0.5449286699295044
two parameters  0.4961511492729187 -0.7878304123878479
minibatch 39
loss =  0.548615038394928
two parameters  0.49257707595825195 -0.7888720035552979
minibatch 40
loss =  0.5616910457611084
two parameters  0.48882758617401123 -0.7897759675979614
minibatch 41
loss =  0.5302520990371704
two parameters  0.48491722345352173 -0.7905540466308594
minibatch 42
loss =  0.5500919818878174
two parameters  0.48085951805114746 -0.7912169098854065
minibatch 43
loss =  0.5492711067199707
two parameters  0.4766671359539032 -0.791774332523346
minibatch 44
loss =  0.5404001474380493
two parameters  0.4723518192768097 -0.792235255241394
minibatch 45
loss =  0.5570074915885925
two parameters  0.4679245948791504 -0.7926077842712402
minibatch 46
loss =  0.5530406832695007
two parameters  0.46339571475982666 -0.7928993701934814
minibatch 47
loss =  0.5454114675521851
two parameters  0.45877474546432495 -0.7931168079376221
minibatch 48
loss =  0.5544111728668213
two parameters  0.45407068729400635 -0.7932662963867188
minibatch 49
loss =  0.5380574464797974
two parameters  0.4492919147014618 -0.7933534979820251
minibatch 50
loss =  0.5452476739883423
two parameters  0.44444623589515686 -0.7933835983276367
minibatch 51
loss =  0.5578618049621582
two parameters  0.4395410418510437 -0.7933613061904907
minibatch 52
loss =  0.5549945831298828
two parameters  0.4345831871032715 -0.793290913105011
minibatch 53
loss =  0.547561526298523
two parameters  0.42957910895347595 -0.7931764125823975
minibatch 54
loss =  0.5316265225410461
two parameters  0.4245348572731018 -0.7930213809013367
minibatch 55
loss =  0.5607225298881531
two parameters  0.4194560647010803 -0.7928290963172913
minibatch 56
loss =  0.5488200783729553
two parameters  0.4143480956554413 -0.7926025390625
minibatch 57
loss =  0.545480489730835
two parameters  0.40921589732170105 -0.7923444509506226
minibatch 58
loss =  0.5585941076278687
two parameters  0.4040641486644745 -0.7920572757720947
minibatch 59
loss =  0.5385826230049133
two parameters  0.39889729022979736 -0.7917433381080627
minibatch 60
loss =  0.5554593205451965
two parameters  0.3937194049358368 -0.7914047241210938
minibatch 61
loss =  0.5518887042999268
two parameters  0.38853442668914795 -0.7910432815551758
minibatch 62
loss =  0.5496640801429749
two parameters  0.38334599137306213 -0.7906607389450073
minibatch 63
loss =  0.5386044383049011
two parameters  0.37815752625465393 -0.7902587056159973
minibatch 64
loss =  0.5400495529174805
two parameters  0.3729722499847412 -0.7898386120796204
minibatch 65
loss =  0.5464751720428467
two parameters  0.3677932322025299 -0.7894017696380615
minibatch 66
loss =  0.5475351810455322
two parameters  0.36262333393096924 -0.7889493107795715
minibatch 67
loss =  0.5498448610305786
two parameters  0.3574652671813965 -0.7884823679924011
minibatch 68
loss =  0.533392608165741
two parameters  0.3523215353488922 -0.7880019545555115
minibatch 69
loss =  0.5534606575965881
two parameters  0.3471945524215698 -0.7875089645385742
minibatch 70
loss =  0.5288681387901306
two parameters  0.3420865833759308 -0.7870042324066162
minibatch 71
loss =  0.5317190289497375
two parameters  0.3369997441768646 -0.7864884734153748
minibatch 72
loss =  0.5509130358695984
two parameters  0.3319360315799713 -0.7859624624252319
minibatch 73
loss =  0.5372831225395203
two parameters  0.3268973231315613 -0.7854267358779907
minibatch 74
loss =  0.5454631447792053
two parameters  0.3218854069709778 -0.7848818898200989
minibatch 75
loss =  0.5435336828231812
two parameters  0.31690195202827454 -0.7843285202980042
minibatch 76
loss =  0.5592387914657593
two parameters  0.3119485080242157 -0.78376704454422
minibatch 77
loss =  0.5654757618904114
two parameters  0.30702656507492065 -0.7831978797912598
minibatch 78
loss =  0.5447889566421509
two parameters  0.3021375238895416 -0.782621443271637
minibatch 79
loss =  0.5494956970214844
two parameters  0.29728269577026367 -0.782038152217865
minibatch 80
loss =  0.557635486125946
two parameters  0.2924633026123047 -0.7814483046531677
minibatch 81
loss =  0.5700458884239197
two parameters  0.2876804769039154 -0.780852198600769
minibatch 82
loss =  0.5394315123558044
two parameters  0.28293532133102417 -0.7802501320838928
minibatch 83
loss =  0.546026885509491
two parameters  0.2782288193702698 -0.7796423435211182
minibatch 84
loss =  0.5501839518547058
two parameters  0.273561954498291 -0.7790290713310242
minibatch 85
loss =  0.5269024968147278
two parameters  0.26893559098243713 -0.7784105539321899
minibatch 86
loss =  0.5526337623596191
two parameters  0.264350563287735 -0.7777869701385498
minibatch 87
loss =  0.5393891334533691
two parameters  0.25980764627456665 -0.7771584987640381
minibatch 88
loss =  0.5379912257194519
two parameters  0.25530755519866943 -0.7765253186225891
minibatch 89
loss =  0.5406066179275513
two parameters  0.25085094571113586 -0.7758876085281372
minibatch 90
loss =  0.5706276297569275
two parameters  0.2464384287595749 -0.7752454876899719
minibatch 91
loss =  0.5418908596038818
two parameters  0.24207058548927307 -0.7745990753173828
minibatch 92
loss =  0.5432615876197815
two parameters  0.2377479374408722 -0.7739484906196594
minibatch 93
loss =  0.5517848134040833
two parameters  0.23347094655036926 -0.7732939124107361
minibatch 94
loss =  0.5472585558891296
two parameters  0.2292400747537613 -0.7726354002952576
minibatch 95
loss =  0.5505102276802063
two parameters  0.22505570948123932 -0.7719730138778687
minibatch 96
loss =  0.5420110821723938
two parameters  0.220918208360672 -0.7713068723678589
minibatch 97
loss =  0.5380082130432129
two parameters  0.2168278992176056 -0.7706370949745178
minibatch 98
loss =  0.5450266599655151
two parameters  0.21278506517410278 -0.7699636816978455
minibatch 99
loss =  0.5507952570915222
two parameters  0.20878995954990387 -0.7692867517471313
minibatch 100
loss =  0.5502752065658569
two parameters  0.20484280586242676 -0.7686063647270203
minibatch 101
loss =  0.5529353022575378
two parameters  0.20094379782676697 -0.7679226398468018
minibatch 102
loss =  0.5477310419082642
two parameters  0.19709308445453644 -0.7672355771064758
minibatch 103
loss =  0.5501365661621094
two parameters  0.1932907998561859 -0.7665452361106873
minibatch 104
loss =  0.553172767162323
two parameters  0.18953704833984375 -0.7658516764640808
minibatch 105
loss =  0.5490114688873291
two parameters  0.18583188951015472 -0.7651549577713013
minibatch 106
loss =  0.5469369292259216
two parameters  0.1821753829717636 -0.7644550800323486
minibatch 107
loss =  0.5677202939987183
two parameters  0.1785675585269928 -0.7637521624565125
minibatch 108
loss =  0.5603106617927551
two parameters  0.1750084012746811 -0.7630462050437927
minibatch 109
loss =  0.5326077342033386
two parameters  0.1714978814125061 -0.7623372673988342
epoch 1
torch.Size([512, 50])
cpu
cpu
cpu 

minibatch 0
loss =  0.5476500988006592
two parameters  0.16803596913814545 -0.761625349521637
minibatch 1
loss =  0.5546727776527405
two parameters  0.16462257504463196 -0.7609105110168457
minibatch 2
loss =  0.5509885549545288
two parameters  0.16125762462615967 -0.7601928114891052
minibatch 3
loss =  0.5440330505371094
two parameters  0.15794099867343903 -0.7594722509384155
minibatch 4
loss =  0.5414280295372009
two parameters  0.15467257797718048 -0.7587488889694214
minibatch 5
loss =  0.5487520098686218
two parameters  0.1514521986246109 -0.7580227255821228
minibatch 6
loss =  0.5422598123550415
two parameters  0.14827971160411835 -0.7572938203811646
minibatch 7
loss =  0.5515803098678589
two parameters  0.1451549232006073 -0.7565621733665466
minibatch 8
loss =  0.5472813248634338
two parameters  0.14207763969898224 -0.7558278441429138
minibatch 9
loss =  0.5429472327232361
two parameters  0.13904763758182526 -0.7550908327102661
minibatch 10
loss =  0.5462498664855957
two parameters  0.13606469333171844 -0.7543511390686035
minibatch 11
loss =  0.551163375377655
two parameters  0.1331285536289215 -0.7536088228225708
minibatch 12
loss =  0.5455839037895203
two parameters  0.13023896515369415 -0.7528639435768127
minibatch 13
loss =  0.5531970858573914
two parameters  0.1273956447839737 -0.7521165013313293
minibatch 14
loss =  0.5638142228126526
two parameters  0.12459831684827805 -0.7513664960861206
minibatch 15
loss =  0.555753231048584
two parameters  0.12184668332338333 -0.7506139874458313
minibatch 16
loss =  0.5600009560585022
two parameters  0.11914043128490448 -0.7498589754104614
minibatch 17
loss =  0.5464272499084473
two parameters  0.11647923290729523 -0.749101459980011
minibatch 18
loss =  0.5446316003799438
two parameters  0.1138627678155899 -0.7483415007591248
minibatch 19
loss =  0.5496060252189636
two parameters  0.11129068583250046 -0.7475790977478027
minibatch 20
loss =  0.5372695922851562
two parameters  0.10876263678073883 -0.7468142509460449
minibatch 21
loss =  0.5426461696624756
two parameters  0.10627825558185577 -0.7460470199584961
minibatch 22
loss =  0.5513410568237305
two parameters  0.10383717715740204 -0.7452774047851562
minibatch 23
loss =  0.5574681162834167
two parameters  0.10143902152776718 -0.7445054650306702
minibatch 24
loss =  0.5491068363189697
two parameters  0.09908340126276016 -0.7437311410903931
minibatch 25
loss =  0.5448879599571228
two parameters  0.09676992148160934 -0.7429545521736145
minibatch 26
loss =  0.5459273457527161
two parameters  0.09449818730354309 -0.7421756386756897
minibatch 27
loss =  0.5422536730766296
two parameters  0.09226778149604797 -0.7413944602012634
minibatch 28
loss =  0.543220579624176
two parameters  0.09007829427719116 -0.7406110167503357
minibatch 29
loss =  0.5463463664054871
two parameters  0.08792930841445923 -0.7398253083229065
minibatch 30
loss =  0.5513296723365784
two parameters  0.08582039922475815 -0.7390373945236206
minibatch 31
loss =  0.5400445461273193
two parameters  0.0837511345744133 -0.738247275352478
minibatch 32
loss =  0.5538768768310547
two parameters  0.08172108232975006 -0.7374549508094788
minibatch 33
loss =  0.543941080570221
two parameters  0.07972980290651321 -0.7366604804992676
minibatch 34
loss =  0.5467435717582703
two parameters  0.07777684926986694 -0.7358638644218445
minibatch 35
loss =  0.5494731068611145
two parameters  0.07586178183555603 -0.7350651025772095
minibatch 36
loss =  0.5325051546096802
two parameters  0.07398414611816406 -0.7342641949653625
minibatch 37
loss =  0.5472735166549683
two parameters  0.07214349508285522 -0.7334612011909485
minibatch 38
loss =  0.5558376908302307
two parameters  0.07033936679363251 -0.7326561212539673
minibatch 39
loss =  0.5597018599510193
two parameters  0.0685713067650795 -0.731848955154419
minibatch 40
loss =  0.5482155680656433
two parameters  0.06683885306119919 -0.7310397624969482
minibatch 41
loss =  0.5464494824409485
two parameters  0.06514154374599457 -0.7302285432815552
minibatch 42
loss =  0.541722297668457
two parameters  0.06347891688346863 -0.7294152975082397
minibatch 43
loss =  0.5342050790786743
two parameters  0.06185051053762436 -0.728600025177002
minibatch 44
loss =  0.5383121371269226
two parameters  0.06025586277246475 -0.7277827858924866
minibatch 45
loss =  0.5495529770851135
two parameters  0.0586945042014122 -0.7269635796546936
minibatch 46
loss =  0.5443412065505981
two parameters  0.0571659654378891 -0.726142406463623
minibatch 47
loss =  0.5452102422714233
two parameters  0.05566978454589844 -0.7253192663192749
minibatch 48
loss =  0.5463658571243286
two parameters  0.05420549586415291 -0.724494218826294
minibatch 49
loss =  0.5594108700752258
two parameters  0.052772633731365204 -0.7236672639846802
minibatch 50
loss =  0.5503892302513123
two parameters  0.051370736211538315 -0.7228384017944336
minibatch 51
loss =  0.5516839623451233
two parameters  0.049999333918094635 -0.722007691860199
minibatch 52
loss =  0.5526505708694458
two parameters  0.048657964915037155 -0.7211750745773315
minibatch 53
loss =  0.5621800422668457
two parameters  0.047346170991659164 -0.7203406095504761
minibatch 54
loss =  0.5396273136138916
two parameters  0.046063486486673355 -0.7195043563842773
minibatch 55
loss =  0.5350591540336609
two parameters  0.044809456914663315 -0.7186662554740906
minibatch 56
loss =  0.537934422492981
two parameters  0.043583620339632034 -0.7178263664245605
minibatch 57
loss =  0.5530357956886292
two parameters  0.0423855260014534 -0.7169846892356873
minibatch 58
loss =  0.5421332716941833
two parameters  0.041214719414711 -0.7161412239074707
minibatch 59
loss =  0.5478125214576721
two parameters  0.04007074981927872 -0.7152960300445557
minibatch 60
loss =  0.5436656475067139
two parameters  0.03895316645503044 -0.7144490480422974
minibatch 61
loss =  0.5578511953353882
two parameters  0.037861526012420654 -0.7136003375053406
minibatch 62
loss =  0.5616945624351501
two parameters  0.03679538518190384 -0.7127499580383301
minibatch 63
loss =  0.5517092943191528
two parameters  0.03575430437922478 -0.7118978500366211
minibatch 64
loss =  0.5652816295623779
two parameters  0.03473784402012825 -0.7110440731048584
minibatch 65
loss =  0.5438952445983887
two parameters  0.033745571970939636 -0.710188627243042
minibatch 66
loss =  0.5600484013557434
two parameters  0.032777056097984314 -0.7093315124511719
minibatch 67
loss =  0.5474385619163513
two parameters  0.03183187171816826 -0.708472728729248
minibatch 68
loss =  0.5475565195083618
two parameters  0.030909594148397446 -0.7076123356819153
minibatch 69
loss =  0.5362737774848938
two parameters  0.03000980243086815 -0.7067503333091736
minibatch 70
loss =  0.5467298626899719
two parameters  0.02913207933306694 -0.705886721611023
minibatch 71
loss =  0.5537673234939575
two parameters  0.02827601321041584 -0.7050215005874634
minibatch 72
loss =  0.5329111218452454
two parameters  0.027441194280982018 -0.7041547298431396
minibatch 73
loss =  0.5536907911300659
two parameters  0.02662721835076809 -0.703286349773407
minibatch 74
loss =  0.5463988780975342
two parameters  0.02583368495106697 -0.7024164199829102
minibatch 75
loss =  0.5578205585479736
two parameters  0.025060195475816727 -0.701545000076294
minibatch 76
loss =  0.5512707829475403
two parameters  0.02430635876953602 -0.7006720304489136
minibatch 77
loss =  0.5623377561569214
two parameters  0.023571787402033806 -0.6997975707054138
minibatch 78
loss =  0.5297114253044128
two parameters  0.0228560958057642 -0.6989216208457947
minibatch 79
loss =  0.556300163269043
two parameters  0.022158905863761902 -0.6980441808700562
minibatch 80
loss =  0.5459474921226501
two parameters  0.021479841321706772 -0.6971652507781982
minibatch 81
loss =  0.5399357676506042
two parameters  0.02081853151321411 -0.6962848901748657
minibatch 82
loss =  0.5392648577690125
two parameters  0.020174609497189522 -0.6954030394554138
minibatch 83
loss =  0.5380299091339111
two parameters  0.019547713920474052 -0.6945197582244873
minibatch 84
loss =  0.5597872734069824
two parameters  0.0189374890178442 -0.693635106086731
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: got SIGCONT
slurmstepd: error: *** STEP 23758240.1 ON gpu62 CANCELLED AT 2021-04-08T13:24:33 ***
slurmstepd: error: *** JOB 23758240 ON gpu62 CANCELLED AT 2021-04-08T13:24:33 ***
srun: forcing job termination
srun: error: gpu62: task 0: Terminated
